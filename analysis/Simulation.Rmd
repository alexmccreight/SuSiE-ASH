---
title: "SuSiE-ASH Simulation"
output: html_document
date: "2024-03-20"
---

```{r, include = F}
# Load Libraries
library(tidyverse)
library(mr.ash.alpha)
library(susieR)
library(FamilyRank)

# Fix File path later
devtools::load_all('/Users/alexmccreight/Columbia/Research/SuSiE-ASH/SuSiE-ASH/submodules/susieR')
devtools::load_all('/Users/alexmccreight/Columbia/Research/SuSiE-ASH/SuSiE-ASH/submodules/mr.ash.alpha')

#devtools::install_github("stephenslab/mr.ash.alpha")

# SuSiE-ASH (v2; Gao's Version)
source("/Users/alexmccreight/Columbia/Research/SuSiE-ASH/SuSiE-ASH/code/SuSiE-ASH-Versions/susie-ash-warmstart-v2.R")
```

# Simulated Data

```{r}
generate_data <- function(n, p, total_heritability, sparse_coverage, nonsparse_coverage, ratio) {
  
  # Generate sparse effects (beta.true)
  num_sparse <- round(p * sparse_coverage)
  beta.true <- rep(0, p)
  beta.true[sample(p, num_sparse)] <- rnorm(num_sparse, mean = 0, sd = 0.4)
  
  # Generate nonsparse effects (theta.true)
  num_nonsparse <- round(p * nonsparse_coverage)
  theta.true <- rep(0, p)
  theta.true[sample(p, num_nonsparse)] <- rnorm(num_nonsparse, mean = 0, sd = 0.01)
  
  # Create Design Matrix
  X <- cbind(matrix(rnorm(n*p),nrow=n))
  X <- scale(X, center=TRUE, scale=FALSE)
  
  # Calculate the variance of the sparse and nonsparse effects
  var_beta <- var(X %*% beta.true)
  var_theta <- var(X %*% theta.true)
  
  # Adjust the effect sizes based on the theta_beta_ratio
  ratio_factor <- (ratio * var_beta) / var_theta
  theta.true <- theta.true * sqrt(ratio_factor)
  
  # Recalculate the variance of the adjusted nonsparse effects
  var_theta_adjusted <- var(X %*% theta.true)
  
  # Calculate the residual variance based on the total heritability
  sigmasq_error <- (var_beta + var_theta_adjusted) * (1 - total_heritability) / total_heritability
  
  # Create Outcomes
  y <- X %*% matrix(beta.true,ncol=1) + X %*% matrix(theta.true,ncol=1) + rnorm(n,0,sqrt(sigmasq_error))
  y <- scale(y, center=TRUE, scale=FALSE)
  
  # Store Information
  return(list(X = X, y = y, error = sigmasq_error, beta = beta.true, theta = theta.true))
}

data <- generate_data(n = 10000, p = 500, total_heritability = 0.25, sparse_coverage = 0.01, nonsparse_coverage = 0.1, ratio = 0.5)

plot(data$beta, ylim = c(-1.5,1.5), ylab = "beta")
plot(data$theta, ylim = c(-1.5,1.5), ylab = "theta")
```

```{r}
generate_data <- function(X, total_heritability, sparse_coverage, nonsparse_coverage, ratio) {
  n <- nrow(X)
  p <- ncol(X)
  
  # Generate sparse effects (beta.true)
  num_sparse <- round(p * sparse_coverage)
  beta.true <- rep(0, p)
  beta.true[sample(p, num_sparse)] <- rnorm(num_sparse, mean = 0, sd = 0.5)
  
  # Generate nonsparse effects (theta.true)
  num_nonsparse <- round(p * nonsparse_coverage)
  theta.true <- rep(0, p)
  theta.true[sample(p, num_nonsparse)] <- rnorm(num_nonsparse, mean = 0, sd = 0.01)
  
  # Scale the genotype matrix
  X <- scale(X, center = TRUE, scale = TRUE)
  
  # Calculate the variance of the sparse and nonsparse effects
  var_beta <- var(X %*% beta.true)
  var_theta <- var(X %*% theta.true)
  
  # Adjust the effect sizes based on the theta_beta_ratio
  ratio_factor <- (ratio * var_beta) / var_theta
  theta.true <- theta.true * sqrt(ratio_factor)
  
  # Recalculate the variance of the adjusted nonsparse effects
  var_theta_adjusted <- var(X %*% theta.true)
  
  # Calculate the residual variance based on the total heritability
  sigmasq_error <- (var_beta + var_theta_adjusted) * (1 - total_heritability) / total_heritability
  
  # Create Outcomes
  y <- X %*% matrix(beta.true, ncol = 1) + X %*% matrix(theta.true, ncol = 1) + rnorm(n, 0, sqrt(sigmasq_error))
  y <- scale(y, center = TRUE, scale = FALSE)
  
  # Store Information
  return(list(X = X, y = y, error = sigmasq_error, beta = beta.true, theta = theta.true))
}


data <- generate_data(X = X[[20]], total_heritability = 0.25, sparse_coverage = 0.0005, nonsparse_coverage = 0.1, ratio = 0.5)

plot(data$beta, ylim = c(-1.5,1.5), ylab = "beta")
plot(data$theta, ylim = c(-1.5,1.5), ylab = "theta")

#sum(data$beta != 0) + sum(data$theta != 0)
```






```{r}
output <- susie_ash_warmstart(X = data$X, y = data$y, L = 10, tol = 0.02, warm_start = 5,  intercept = F, standardize = F)
output2 <- susie(X = data$X, y = data$y, L = 10, intercept = F, standardize = F)#, tol = 0.001)

#output <- susie_ash_warmstart(X = data$X, y = data$y, L = 10, tol = 0.003)
# output2 <- susie(X = data$X, y = data$y, L = 10)

susie_get_cs_attainable(output)
susie_get_cs(output2)

# output3 <- susie_ash_warmstart_re(X = data$X, y = data$y, L = 10, tol = 0.02)
# 
# susie_get_cs(output, X = data$X)
# 

output$sets
output2$sets
beta.true.index = which(data$beta != 0)
theta.true.index =  which(data$theta != 0)
#theta.true.index = which(abs(data$theta) >= 0.0025)

# Color + Shape Differ for True Effects
color_vector <- rep("black", length(output2$pip))
color_vector[beta.true.index] <- "red"
shape_vector <- rep(1, length(output2$pip))
shape_vector[beta.true.index] <- 17

color_vector[theta.true.index] <- "blue"
shape_vector[theta.true.index] <- 16
# 
output$pip %>% plot(., main = "SuSiE-ASH (v2) PIP", col = color_vector, pch = shape_vector) %>% abline(h = 0.9, col="black", lty = 2) 
output2$pip %>% plot(., main = "SuSiE PIP", col = color_vector, pch = shape_vector) %>% abline(h = 0.9, col="black", lty = 2)
# output3$pip %>% plot(., main = "SuSiE-ASH (v1) PIP", col = color_vector, pch = shape_vector) %>% abline(h = 0.9, col="black", lty = 2) 

# Use points() to bring the red
```



# Methods (SuSiE, SuSiE-ASH Variants)

```{r}
method_and_score <- function(X = data$X, y = data$y, beta = data$beta, theta = data$theta, L = 10, threshold = 0.9) {
  # Run various methods
  susie_output <- susie(X = X, y = y, L = L)
  susie_ash_output <- susie_ash_warmstart(X = X, y = y, L = L, warm_start = 5, tol = 0.03)
  
  
  calc_metrics <- function(mod, beta = data$beta, theta = data$theta, threshold = 0.9) {
    # Identify causal variables (non-zero sparse effects)
    causal <- beta != 0 | theta != 0
    
    # Identify causal variables as non-zero sparse effects and infinitesimal effects with magnitude > 0.03
    #causal <- (beta != 0) | (abs(theta) > 0.03)
    significant <- mod$pip > threshold
    
    # Calculate FDR
    fdr <- ifelse(sum(significant) > 0, sum(!causal & significant) / sum(significant), 0)
    
    # Calculate Recall
    recall <- ifelse(sum(causal) > 0, sum(causal & significant) / sum(causal), 0)
    
    # Calculate Average CS Size
    cs_size <- length(unlist(susie_get_cs(mod, X = data$X, coverage = threshold)$cs)) / length(susie_get_cs(mod, X = data$X, coverage = threshold)$cs)
    
    # Calculate Coverage (proportion of cs with causal effect)
    coverage <- sum(which(causal) %in% susie_get_cs(mod, X = data$X, coverage = threshold)$cs) / length(susie_get_cs(mod, X = data$X, coverage = threshold)$cs)
    
    # Calibration
    calibration <- ifelse(sum(causal) > 0, sum(causal & significant) / sum(significant), 0)
    
    # CS Based FDR and Recall
    # test.true <- which(data$X != 0)
    # test.cs <- susie_get_cs(mod, X = data$X)$cs
    # 
    #   TP = sum(test.true %in% unlist(test.cs))
    #   FN = length(test.true) - TP
    #   FP = length(test.cs) - lapply(1:length(test.cs), function(cs.l){ ifelse(sum(test.cs[[cs.l]] %in%test.true)!=0,T,F)}) %>% unlist(.) %>% sum(.) # FP based on CSs
    #   FP = length(unlist(test.cs)) - sum(unlist(test.cs) %in% test.true) # FP based on elements
    #   
    #   cs_fdr = FP/(TP+FP)
    #   cs_recall = TP/(TP+FN)

    
    return(list(fdr = fdr, recall = recall, cs_size = cs_size, coverage = coverage, calibration = calibration))#, cs_fdr = cs_fdr, cs_recall = cs_recall))
  }
  
  # Calculate FDR and Recall for each method
  susie_metrics <- calc_metrics(susie_output, beta, theta, threshold)
  susie_ash_output <- calc_metrics(susie_ash_output, beta, theta, threshold)
  
  #Create a data frame with the results
  metrics_table  <- data.frame(
    Model = c("SuSiE", "SuSiE-ASH"),
    FDR = c(susie_metrics$fdr, susie_ash_output$fdr),
    Recall = c(susie_metrics$recall, susie_ash_output$recall),
    CS_Size = c(susie_metrics$cs_size, susie_ash_output$cs_size),
    Coverage = c(susie_metrics$coverage, susie_ash_output$coverage),
    Calibration = c(susie_metrics$calibration, susie_ash_output$calibration)
    #CS_FDR = c(susie_metrics$cs_fdr, susie_ash_v1_metrics$cs_fdr, susie_ash_v2_metrics$cs_fdr),
    #CS_Recall = c(susie_metrics$cs_recall, susie_ash_v1_metrics$cs_recall, susie_ash_v2_metrics$cs_recall)
  )
  # Return the results table
  return(list(
    metrics = metrics_table,
    susie_output = susie_output,
    susie_ash_output = susie_ash_output,
    betas = data$beta,
    thetas = data$theta)
  )
}
```


# Scores (FDR, Recall)

```{r}
simulate_and_score <- function(num_simulations = 10, n = 10000, p = 500, heritability = 0.75, sparse_coverage = 0.01, nonsparse_coverage = 0.1, L = 10, threshold = 0.9) {
  
  # Initialize lists to store results
  all_metrics <- list()
  all_betas <- list()
  all_thetas <- list()
  all_susie_outputs <- list()
  all_susie_ash_outputs_v1 <- list()
  all_susie_ash_outputs_v2 <- list()
  all_seeds <- numeric(num_simulations)
  
  for (i in 1:num_simulations) {
    cat("Running simulation", i, "out of", num_simulations, "\n")
    
    # Set random seed for each simulation
    seed <- abs(round(rnorm(1, mean = 0, sd = 1000)))
    set.seed(seed)
    
    # Generate data
    data <- generate_data(n, p, heritability, sparse_coverage, nonsparse_coverage)
    
    # Run methods and calculate metrics
    results <- method_and_score(X = data$X, y = data$y, beta = data$beta, L = L, threshold = threshold)
    
    # Store results + betas/thetas
    all_metrics[[i]] <- results$metrics
    all_betas[[i]] <- data$beta
    all_thetas[[i]] <- data$theta
    all_susie_outputs[[i]] <- results$susie_output
    all_susie_ash_outputs_v1[[i]] <- results$susie_ash_output_v1
    all_susie_ash_outputs_v2[[i]] <- results$susie_ash_output_v2
    all_seeds[i] <- seed
  }
  
  # Calculate average metrics
  avg_metrics <- data.frame(
    Model = unique(all_metrics[[1]]$Model),
    FDR = Reduce("+", lapply(all_metrics, function(x) x$FDR)) / num_simulations,
    Recall = Reduce("+", lapply(all_metrics, function(x) x$Recall)) / num_simulations,
    CS_Size = Reduce("+", lapply(all_metrics, function(x) x$CS_Size)) / num_simulations,
    Coverage = Reduce("+", lapply(all_metrics, function(x) x$Coverage)) / num_simulations,
    Calibration = Reduce("+", lapply(all_metrics, function(x) x$Calibration)) / num_simulations
    #CS_FDR = Reduce("+", lapply(all_metrics, function(x) x$CS_FDR)) / num_simulations,
    #CS_Recall = Reduce("+", lapply(all_metrics, function(x) x$CS_Recall)) / num_simulations
  )
  
  # Return all results
  return(list(
    avg_metrics = avg_metrics,
    all_metrics = all_metrics,
    all_betas = all_betas,
    all_thetas = all_thetas,
    all_susie_outputs = all_susie_outputs,
    all_susie_ash_outputs_v1 = all_susie_ash_outputs_v1,
    all_susie_ash_outputs_v2 = all_susie_ash_outputs_v2,
    all_seeds = all_seeds
  ))
}


# large_simulation <- simulate_and_score(num_simulations = 10, n = 10000, p = 500, heritability = 0.55, sparse_coverage = 0.01, nonsparse_coverage = 0.1, L = 10, threshold = 0.9)
# 
# large_simulation2 <- simulate_and_score(num_simulations = 10, n = 10000, p = 500, heritability = 0.75, sparse_coverage = 0.01, nonsparse_coverage = 0.1, L = 10, threshold = 0.9)
# 
# large_simulation3 <- simulate_and_score(num_simulations = 10, n = 10000, p = 500, heritability = 0.88, sparse_coverage = 0.01, nonsparse_coverage = 0.1, L = 10, threshold = 0.9)

20*55


# print(large_simulation$avg_metrics)
# print(large_simulation2$avg_metrics)
# print(large_simulation3$avg_metrics)


# CS-Size will change with real data


```

Note-to-self: 
- Recall/Power = Percentage of simulated LARGE EFFECTS among the top N variants when ranked by PIP (Note that recall for SuSiE-Inf was very similar to, but slightly lower than the recall of SuSiE)
- False Discovery Rate = P(Non-Causal | PIP > 0.9)
- Calibration = Of variants with PIP = x%, we expect x% are truly causal
- Replication Failure Rate = P(PIP Large Sample < 0.1 | PIP Small Sample > 0.9)
- Expected Proportion of Non-Causal Variants (EPN) --> Supp table 6,7,8
"If the RFR is significantly higher than the EPN, it suggests miscalibration"
- Coverage: proportion of variants with nonzero effects
- Heritability: proportion of outcome variance that can be attributed to genetic effects (our betas and thetas)


To implement: 
1) CS Size, similar to precision (complete)

2) Coverage = proportion of credible sets with a causal effect, 95% coverage want CS with 95% prob of containing a causal effect (complete, credible set-based calibration from susie-inf?)

3) ROC curve with FDR vs Power, way of comparing PIP

4) Coverage versus L (from susie) graph 

5) How does coverage change when we vary heritability versus the amount of actual simulated effects


